import re
from typing import Tuple, Iterable
# -------------------------------------------------------------------
# Ultra-optimized Wake-word Detector (Class Version) - NO Levenshtein!
# -------------------------------------------------------------------

class WakeWordDetector:
    """
    Wake-word detector Ù…ÙØ­Ø³Ù‘ÙÙ† Ù„Ù„Ø£Ø¯Ø§Ø¡ (Ø¨Ø¯ÙˆÙ† Levenshtein).
    ÙŠØ¯Ø¹Ù…:
      - ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ Ø®ÙÙŠÙ
      - Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ù†Ø¯Ø§Ø¡ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù†Øµ ÙÙ‚Ø·
      - Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù‚Ø¨ÙˆÙ„/Ø±ÙØ¶ O(1) lookup
    """

    # ØªØ­ÙŠÙ‘Ø§Øª (Ù…Ù…ÙƒÙ† ØªØ­ØªØ§Ø¬Ù‡Ø§ Ø®Ø§Ø±Ø¬ÙŠÙ‹Ø§ â€” ØªØ±ÙƒÙ†Ø§Ù‡Ø§ ÙƒÙ€ class attrs)
    GREETING_AR = ["Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§", "Ø£Ù‡Ù„Ø§", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù‡Ù„Ø§", "Ø§Ù‡Ù„ÙŠÙ†", "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±"]
    GREETING_EN = ["hello", "hi", "hey", "good morning", "good afternoon", "good evening", "howdy"]

    # -------- Arabic Diacritics (precompiled) --------
    _AR_DIACRITICS = re.compile(r'[\u0617-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]')

    def __init__(
        self,
        ar_wake_word: str = "Ø²ÙŠÙƒÙˆ",
        en_wake_exact: Iterable[str] = (
            "ziko", "zico", "zeeko", "zeeco",
            "zikko", "zeiko", "zyko", "zeko",
            "dziko", "dico", "zika", "nico", "niko", "echo"
             
        ),
        en_wake_deny: Iterable[str] = (
            #"zika", "nico", "nika", "nikaa",
           # "z", "d",  # Ø­Ø±ÙˆÙ Ù…ÙØ±Ø¯Ø©
        ),
    ):
        """
        :param ar_wake_word: ÙƒÙ„Ù…Ø© Ø§Ù„Ù†Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§: Ø²ÙŠÙƒÙˆ)
        :param en_wake_exact: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ (frozenset O(1))
        """
        self.ar_wake_word = ar_wake_word
        self._EN_WAKE_EXACT = frozenset(x.lower() for x in en_wake_exact)
        self._EN_WAKE_DENY = frozenset(x.lower() for x in en_wake_deny)

        # -------- Regex ØªÙØ¨Ù†Ù‰ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© --------
        # Ø¹Ø±Ø¨ÙŠ: ^\s*(?:ÙŠØ§\s*)?{ar_wake}\b[\sØŒ,:-]*
        ar_escaped = re.escape(self._normalize_ar(ar_wake_word))
        self._AR_WAKE_REGEX = re.compile(
            rf"^\s*(?:ÙŠØ§\s*)?{ar_escaped}\b[\sØŒ,:-]*",
            re.IGNORECASE
        )

        # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ: Ù†Ù„ØªÙ‚Ø· Ø£ÙˆÙ„ ÙƒÙ„Ù…Ø© ÙÙ‚Ø· ÙƒØ¨Ø§Ø¯Ø¦Ø©
        self._EN_WAKE_REGEX = re.compile(
            r"^\s*(?:hey|hi|hello\s+)?([a-z]+)[\s,ØŒ:.\-!?]*",
            re.IGNORECASE
        )

    # ---------------- Utilities ----------------
    @staticmethod
    def contains_any(text: str, words: list[str]) -> bool:
        """ÙØ­Øµ Ø³Ø±ÙŠØ¹ (O(n)) Ù„ÙˆØ¬ÙˆØ¯ Ø£ÙŠ ÙƒÙ„Ù…Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ (lowercase Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)."""
        if not text:
            return False
        t = text.lower()
        return any((w and w.lower() in t) for w in words)

    # ---------------- Normalization (Arabic) ----------------
    def _normalize_ar(self, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹."""
        if not text:
            return ""
        text = self._AR_DIACRITICS.sub('', text.strip().lower())
        text = text.translate(str.maketrans({
            'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ù±': 'Ø§',
            'Ø©': 'Ù‡', 'Ù‰': 'ÙŠ', 'Ù€': ''
        }))
        return text

    # ---------------- English Wake Token Check ----------------
    def _is_english_wake_token(self, tok: str) -> bool:
        """
        ÙØ­Øµ ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ù„Ù€ wake-token Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ:
          - Ø±ÙØ¶ Ø³Ø±ÙŠØ¹ O(1)
          - Ù‚Ø¨ÙˆÙ„ Ø¯Ù‚ÙŠÙ‚ O(1)
          - Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø© Ø¨Ø¯ÙˆÙ† Levenshtein
        """
        if not tok or len(tok) < 2:
            return False

        t = tok.lower()

        # 1) Ø±ÙØ¶ ÙÙˆØ±ÙŠ
        if t in self._EN_WAKE_DENY:
            return False

        # 2) Ù‚Ø¨ÙˆÙ„ Ø¯Ù‚ÙŠÙ‚
        if t in self._EN_WAKE_EXACT:
            return True

        # 3) Ø´Ø±ÙˆØ· Ø¨Ø³ÙŠØ·Ø© (Ø¨Ø¯ÙˆÙ† Levenshtein):
        #    ÙŠØ¨Ø¯Ø£ Ø¨Ù€ z Ø£Ùˆ d + ÙŠØ­ØªÙˆÙŠ "iko"/"ico"
        if t[0] not in ('z', 'd'):
            return False

        if "iko" in t or "ico" in t:
            return True

        return False

    # ---------------- Main API ----------------
    def extract_after_wake(self, user_text: str) -> Tuple[bool, str, str]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ wake-word Ù…Ø¹ Ø¥Ø±Ø¬Ø§Ø¹: (has_wake, remainder, wake_form)
        - ÙŠØªØ­Ù‚Ù‚ Ø£ÙˆÙ„Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© (Ø£Ø³Ø±Ø¹ Ù…Ø³Ø§Ø±)ØŒ Ø«Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
        - remainder ÙŠÙØ¹Ø§Ø¯ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø¨Ø¯ÙˆÙ† lowercase).
        """
        text = (user_text or "").strip()
        if not text:
            return False, "", ""

        # ===== English Detection (Fast Path) =====
        text_lower = text.lower()
        m_en = self._EN_WAKE_REGEX.match(text_lower)

        if m_en:
            first_token = m_en.group(1)
            if self._is_english_wake_token(first_token):
                return True, text[m_en.end():].strip(), first_token

        # ===== Arabic Detection =====
        norm_ar = self._normalize_ar(text)
        m_ar = self._AR_WAKE_REGEX.match(norm_ar)

        if m_ar:
            # Ù‚Øµ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…ÙƒØ§ÙØ¦Ø©
            # Ù†Ø¹ÙŠØ¯ Ø¨Ù†Ø§Ø¡ Ù†ÙØ³ Ø§Ù„Ù†Ù…Ø· Ù„ÙƒÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ (ØºÙŠØ± Ù…ÙØ·Ø¨Ù‘ÙØ¹)
            orig_ar_regex = re.compile(
                rf"^\s*(?:ÙŠØ§\s*)?{re.escape(self.ar_wake_word)}\b[\sØŒ,:-]*",
                re.IGNORECASE
            )
            m_orig = orig_ar_regex.match(text)
            if m_orig:
                return True, text[m_orig.end():].strip(), m_orig.group(0).strip()

            # fallback Ø¨Ø³ÙŠØ· Ù„Ùˆ Ø§Ù„ØªØ¹Ø§Ø¯Ù„ Ø¨ÙŠÙ† Ø§Ù„ØªØ·Ø¨ÙŠØ¹/Ø§Ù„Ø£ØµÙ„ Ø£Ø®ÙÙ‚
            return True, text[len(self.ar_wake_word):].strip(), self.ar_wake_word

        return False, "", ""

# ================= Demo / Quick Test =================
if __name__ == "__main__":
    import time

    detector = WakeWordDetector()

    print("=" * 70)
    print("ğŸš€ ULTRA-FAST Wake-word Detection (Class Version)")
    print("=" * 70)
    print()

    test_cases = [
        # Expected to WAKE âœ…
        ("Ziko play some music", True, "play some music"),
        ("hey zico open mail", True, "open mail"),
        ("Dico what's the weather", True, "what's the weather"),
        ("Dziko, read my latest email", True, "read my latest email"),
        ("hello ziko, what's up?", True, "what's up?"),
        ("ZEeCo, open settings", True, "open settings"),
        ("Ø²ÙŠÙƒÙˆ: Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±", True, "Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±"),
        ("ÙŠØ§ Ø²ÙŠÙƒÙˆ Ø§ÙØªØ­ Ø§Ù„Ø¨Ø±ÙŠØ¯", True, "Ø§ÙØªØ­ Ø§Ù„Ø¨Ø±ÙŠØ¯"),
        ("ziko", True, ""),

        # Expected to IGNORE âŒ
        ("Nico open calendar", False, ""),
        ("Zika is a virus", False, ""),
        ("Z.", False, ""),
        ("Z", False, ""),
        ("sorry, ziko open mail", False, ""),  # Ù„ÙŠØ³ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        ("play some music", False, ""),
    ]

    print("ğŸ“‹ Functional Tests:")
    print("-" * 70)

    passed = 0
    total_time = 0

    for text, expected_wake, expected_cmd in test_cases:
        start = time.perf_counter()
        has_wake, remainder, wake_form = detector.extract_after_wake(text)
        elapsed = time.perf_counter() - start
        total_time += elapsed

        success = (has_wake == expected_wake)
        if success and expected_wake:
            success = (remainder == expected_cmd)

        status = "âœ…" if success else "âŒ"
        passed += 1 if success else 0

        print(f"{status} '{text[:40]}'")
        if has_wake:
            print(f"   Wake: '{wake_form}' â†’ Command: '{remainder}'")
        print(f"   Time: {elapsed*1000:.4f}ms")
        print()

    print("=" * 70)
    print(f"Results: {passed}/{len(test_cases)} passed ({'âœ…' if passed == len(test_cases) else 'âŒ'})")
    print(f"Average time: {(total_time/len(test_cases))*1000:.4f}ms")
    print()

    # ===== Performance Stress Test =====
    print("âš¡ Performance Stress Test:")
    print("-" * 70)

    test_texts = [
        "Ø²ÙŠÙƒÙˆ Ø§ÙØªØ­ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ",
        "Ziko play some music",
        "hey zico what's the weather",
        "not a wake word at all"
    ]

    for test_text in test_texts:
        iterations = 5000
        start = time.perf_counter()

        for _ in range(iterations):
            detector.extract_after_wake(test_text)

        elapsed = time.perf_counter() - start
        avg_us = (elapsed / iterations) * 1_000_000  # microseconds

        print(f"Text: '{test_text[:35]}'")
        print(f"  {iterations} iterations: {elapsed*1000:.2f}ms")
        print(f"  Average: {avg_us:.2f}Î¼s per call")
        print(f"  Throughput: {iterations/elapsed:.0f} calls/sec")
        print()

    print("=" * 70)
    print("âœ¨ Optimization: NO Levenshtein, O(1) lookups only!")
    print("ğŸ¯ Perfect for Raspberry Pi Zero with minimal CPU usage")
    print("=" * 70)


class StopCommandDetector:
    """
    Detects stop commands in both Arabic and English, with support for normalization
    and optional wake words (e.g., 'Ziko stop', 'Ø²ÙŠÙƒÙˆ ÙˆÙ‚Ù').
    """

    # ------------------- Arabic normalization -------------------
    # Regex to remove Arabic diacritics and elongations.
    _AR_DIACRITICS = re.compile(r'[\u0617-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]')

    # Stop tokens that indicate interruption or stopping the assistant.
    STOP_TOKENS = [
        # English
        "stop", "end", "cancel", "enough", "quit", "exit", "abort", "halt",
        # Arabic (common forms)
        "Ù‚Ù", "ØªÙˆÙ‚Ù", "ÙˆÙ‚Ù", "Ø¨Ø³", "Ø®Ù„Øµ", "Ø®Ù„Ø§Øµ", "ÙƒÙØ§ÙŠÙ‡", "ÙƒÙØ§ÙŠØ©",
        "Ø³ØªÙˆØ¨", "ÙˆÙ‚Ù Ø§Ù„ØªØ´ØºÙŠÙ„", "Ø§Ø³ÙƒØª", "ÙƒÙØ§", "Ø®Ù„ØµÙ†Ø§", "Ø®Ù„Ø§Øµ ÙƒØ¯Ù‡",
    ]

    # Pattern: match stop token at beginning, followed by whitespace, punctuation, or end of string.
    _BOUNDARY = r'(?:\s|$|[^\w\u0600-\u06FF])'

    def __init__(self, extract_after_wake_func=None):
        """
        :param extract_after_wake_func: Optional callable for detecting and extracting
                                        text after a wake word (e.g., 'Ziko stop').
        """
        self.extract_after_wake_func = extract_after_wake_func

        # Build regex once during initialization for better performance
        pattern = r'^\s*(?:' + '|'.join(map(re.escape, self.STOP_TOKENS)) + r')' + self._BOUNDARY
        self._stop_re = re.compile(pattern, re.IGNORECASE)

    # -----------------------------------------------------------------
    #                     Arabic Normalization
    # -----------------------------------------------------------------
    def _normalize_ar(self, text: str) -> str:
        """
        Normalize Arabic text:
        - Lowercase and strip whitespace
        - Remove diacritics and elongations
        - Convert letter variants (Ø£Ø¥Ø¢Ù± â†’ Ø§, Ø© â†’ Ù‡, Ù‰ â†’ ÙŠ)
        """
        if not text:
            return ""
        text = text.strip().lower()
        text = self._AR_DIACRITICS.sub('', text)
        text = text.replace('Ù€', '')  # remove kashida
        for src in 'Ø£Ø¥Ø¢Ù±':
            text = text.replace(src, 'Ø§')
        text = text.replace('Ø©', 'Ù‡')
        text = text.replace('Ù‰', 'ÙŠ')
        return text

    # -----------------------------------------------------------------
    #                     Core Detection Logic
    # -----------------------------------------------------------------
    def is_stop_command(self, text: str) -> bool:
        """
        Returns True if the text begins with a recognized stop command
        in Arabic or English.
        """
        if not text:
            return False

        normalized = self._normalize_ar(text)
        return bool(self._stop_re.search(text) or self._stop_re.search(normalized))

    def is_stop_with_optional_wake(self, text: str) -> bool:
        """
        Returns True if:
        - The text is a stop command directly (e.g., "stop", "ÙˆÙ‚Ù")
        - OR after a wake word (e.g., "Ziko stop", "Ø²ÙŠÙƒÙˆ ÙˆÙ‚Ù")
        """
        if not text:
            return False

        candidate = text
        if self.extract_after_wake_func:
            try:
                has_wake, remainder, _wake = self.extract_after_wake_func(text)
                candidate = remainder if has_wake else text
            except Exception:
                pass

        return self.is_stop_command(candidate)



